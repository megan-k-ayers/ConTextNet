% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/prep_data.R
\name{prep_data}
\alias{prep_data}
\title{Prepare data for ConTextNet model}
\usage{
prep_data(
  x,
  y_name,
  text_name,
  grid_vals,
  task,
  token_method = "default",
  max_length = 200,
  token_instr = NULL,
  embed_method = "default",
  embed_instr = NULL,
  tune_method = "local"
)
}
\arguments{
\item{x}{The input data as a data frame.}

\item{y_name}{The name of the outcome column \code{x}.}

\item{text_name}{The name of the text column in \code{x}.}

\item{grid_vals}{List defining the parameter values to consider during
tuning.}

\item{task}{Whether this is "reg" (regression) or "class" (classification).}

\item{token_method}{How tokenizing should be performed ("file" for embeddings
read from a file, "name" for Hugging Face model referenced by name,
"default" for default BERT sub-word tokenization).}

\item{max_length}{Max number of tokens to consider per text sample. Longer
texts with tokens will be cut off beyond this threshold. Shorter
values of \code{max_length} correspond to a faster model.}

\item{token_instr}{Either a list with (1) the relative file name to read
tokens from and (2) the relative file name to read the vocabulary
list from, or the name of the Hugging Face model to use.}

\item{embed_method}{How embedding should be performed ("file" for embeddings
read from a file, "name" for Hugging Face model referenced by name).}

\item{embed_instr}{Either the relative file name to read embeddings from, or
the name of the Hugging Face model to use.}

\item{tune_method}{How tuning should be performed: locally, via a Cluster
with Slurm, or generically via a shell script.}
}
\description{
Prepare data for ConTextNet model
}
\examples{
\dontrun{res <- prep_data(imdb, "y", "text", list())}
}
