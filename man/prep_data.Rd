% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/prep_data.R
\name{prep_data}
\alias{prep_data}
\title{Prepare data for ConTextNet model}
\usage{
prep_data(
  x,
  y_name,
  text_name,
  model_params,
  task,
  test_prop = 0.2,
  embed_method = "default",
  embed_instr = list(max_length = 200),
  tune_method = "none",
  folder_name
)
}
\arguments{
\item{x}{The input data as a data frame.}

\item{y_name}{The name of the outcome column \code{x}.}

\item{text_name}{The name of the text column in \code{x}.}

\item{model_params}{List defining the parameter values to consider during
tuning.}

\item{task}{Whether this is "reg" (regression) or "class" (classification).}

\item{test_prop}{Proportion of \code{nrow(x)} to reserve in the test set.}

\item{embed_method}{How embedding should be performed ("file" to read from
files, "name" for Hugging Face model referenced by name, or "default"
for a default BERT model).}

\item{embed_instr}{Depending on choice for embed_method, a list containing
The "file" method: the file path to read tokens from, the file path to
read the token vocabulary list from, and the file path to read the
token embeddings from (named "token_path", "vocab_path", and
"embed_path")
The "name" method: the name of the Hugging Face model to use and the
max number of tokens to consider per text sample (named "name" and
"max_length")
The "default" method: the max number of tokens to consider per text
sample (named "max_length")}

\item{tune_method}{How tuning should be performed: locally, via a Cluster
with Slurm, generically via a shell script, or not at all
("local", "slurm", "shell", or "none")}

\item{folder_name}{Name of directory to create for saving model files.}
}
\description{
Prepare data for ConTextNet model
}
\examples{
\dontrun{
model_params <- list("n_filts" = list(2), "kern_sizes" = list(c(3, 5)),
                   "lr" = list(0.0001), "lambda_cnn" = list(0),
                   "lambda_corr" = list(0), "lambda_out" = list(0),
                   "epochs" = list(20), "batch_size" = list(32),
                   "covars" = list(NULL))
res <- prep_data(x = imdb, y_name = "y", text_name = "text",
                 model_params = model_params, task = "class",
                 folder_name = "example")

model_params <- list("n_filts" = 2, "kern_sizes" = c(3, 5), "lr" = 0.0001,
                   "lambda_cnn" = 0, "lambda_corr" = 0, "lambda_out" = 0,
                   "epochs" = 20, "batch_size" = 32, "covars" = NULL)
res <- prep_data(x = imdb, y_name = "y", text_name = "text",
                 model_params = model_params, task = "class",
                 folder_name = "example")

res <- prep_data(x = imdb, y_name = "y", text_name = "text",
                 model_params = model_params, task = "class",
                 embed_method = "name",
                 embed_instr = list("name" = "bert-base-cased",
                                    "max_length" = 200),
                 folder_name = "example")
}
}
