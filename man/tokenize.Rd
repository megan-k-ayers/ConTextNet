% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/tokenize.R
\name{tokenize}
\alias{tokenize}
\title{Tokenize input text}
\usage{
tokenize(x, embed_method = "default", embed_instr = list(max_length = 200))
}
\arguments{
\item{x}{A data frame with a \code{text} column.}

\item{embed_method}{How embedding should be performed ("file" to read from
files, "name" for Hugging Face model referenced by name, or "default"
for a default BERT model).}

\item{embed_instr}{Depending on choice for embed_method, a list containing
The "file" method: the file path to read tokens from, the file path to
read the token vocabulary list from, and the file path to read the
token embeddings from (named "token_path", "vocab_path", and
"embed_path")
The "name" method: the name of the Hugging Face model to use and the
max number of tokens to consider per text sample (named "name" and
"max_length")
The "default" method: the max number of tokens to consider per text
sample (named "max_length")}
}
\value{
A list containing: a matrix with tokens corresponding to \code{x$text},
(\code{tokens}) and a list with the tokenizer vocabulary (\code{vocab}).
}
\description{
Input text is converted to tokens. This can be performed with one of the
following methods:
\enumerate{
\item A default BERT sub-word tokenizer (requires input for \code{max_length})
\item Chosen tokenizer model name from the Hugging Face transformer library
(requires input for \code{tokenizer} and \code{max_length})
\item Existing tokens uploaded from a file (requires inputs for \code{file_name}
and \code{token_map}). Order is assumed to match between \code{x} and the token
data in \code{file_name}.
}
}
\examples{
\dontrun{res <- tokenize(imdb)}
}
